import os
import torch
import discord
from discord.ext import commands
from transformers import pipeline
from PIL import Image
import io
from cogs.premium.premium import PremiumDatabase

class NSFWDetection(commands.Cog):
    def __init__(self, bot):
        device_id = 0 if torch.cuda.is_available() else -1
        self.bot = bot
        self.classifier = pipeline(
            "image-classification",
            model="Falconsai/nsfw_image_detection",
            device=device_id
        )

    @commands.command(name="nsfwdetect")
    async def analyze_nsfw(self, ctx):
        # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç„¡è¦–
        privacy_cog = self.bot.get_cog("Privacy")
        if privacy_cog and privacy_cog.is_private_user(ctx.author.id):
            return

        user_id = ctx.author.id
        premium_db = PremiumDatabase()
        user_data = premium_db.get_user(user_id)
        if not user_data:
            return await ctx.send("ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼å°‚ç”¨ã§ã™ã€‚ãƒ—ãƒ¬ãƒŸã‚¢ãƒ æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã«ã¯ã€Swiftlyã‚’è‡ªåˆ†ã®ã‚µãƒ¼ãƒãƒ¼ã«å°å…¥ã—ã¦ãã ã•ã„ã€‚æ—¢ã«ã‚µãƒ¼ãƒãƒ¼ã«å°å…¥æ¸ˆã¿ã®å ´åˆã¯ã€é–‹ç™ºè€…(techfish_1)ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚\n(ãƒ—ãƒ¬ãƒŸã‚¢ãƒ æ©Ÿèƒ½ã¯å®Œå…¨ç„¡æ–™ã§ã™ã€‚æœ‰æ–™ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚)")

        if not (ctx.message.reference and ctx.message.reference.message_id):
            return await ctx.send("å‚ç…§å…ˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç”»åƒã‚’æ·»ä»˜ã—ã¦ãã ã•ã„ã€‚")

        ref_message = await ctx.channel.fetch_message(ctx.message.reference.message_id)
        analyzing_msg = await ctx.send("è§£æä¸­...")

        valid_attachments = [att for att in ref_message.attachments
                             if att.filename.lower().endswith(('jpg','jpeg','png'))]
        if not valid_attachments:
            return await analyzing_msg.edit(content="ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # Collect all images
        images = []
        for att in valid_attachments:
            image_bytes = await att.read()
            image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
            # Convert to PNG if not already
            if not att.filename.lower().endswith('png'):
                buffer = io.BytesIO()
                image.save(buffer, format="PNG")
                buffer.seek(0)
                image = Image.open(buffer)
            images.append(image)

        # Single batch inference
        results = self.classifier(images)

        final_label = 'SAFE'
        description_lines = []

        for idx, result in enumerate(results, start=1):
            # Directly check the label from the result
            label = 'NSFW' if result[0]['label'] == 'nsfw' else 'SAFE'
            if label == 'NSFW':
                final_label = 'NSFW'
            description_lines.append(
                f"ç”»åƒ {idx}:\n"
                f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å: {valid_attachments[idx-1].filename}\n"
                f"ğŸ” åˆ¤å®šãƒ©ãƒ™ãƒ«: {label} (ä¿¡é ¼åº¦: {result[0]['score']*100:.2f}%)\n\n"
            )

        embed = discord.Embed(
            title="NSFW ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¤å®šçµæœ",
            description=(
                "ç”»åƒã®å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n\n"
                + "\n".join(description_lines)
                + "\næœ€çµ‚åˆ¤å®šçµæœ: " + final_label
            ),
            color=discord.Color.green() if final_label == 'SAFE' else discord.Color.red()
        )
        await analyzing_msg.edit(content=None, embed=embed)

async def setup(bot):
    await bot.add_cog(NSFWDetection(bot))
