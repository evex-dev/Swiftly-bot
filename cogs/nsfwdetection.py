import os
import torch
import discord
from discord.ext import commands
from transformers import pipeline
from PIL import Image
import io

class NSFWDetection(commands.Cog):
    def __init__(self, bot):
        device_id = 0 if torch.cuda.is_available() else -1
        self.bot = bot
        self.classifier = pipeline(
            "image-classification",
            model="Falconsai/nsfw_image_detection",
            device=device_id
        )

    @commands.command(name="nsfwdetect")
    async def analyze_nsfw(self, ctx):
        if not (ctx.message.reference and ctx.message.reference.message_id):
            return await ctx.send("å‚ç…§å…ˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç”»åƒã‚’æ·»ä»˜ã—ã¦ãã ã•ã„ã€‚")

        ref_message = await ctx.channel.fetch_message(ctx.message.reference.message_id)
        analyzing_msg = await ctx.send("è§£æä¸­...")

        valid_attachments = [att for att in ref_message.attachments
                             if att.filename.lower().endswith(('jpg','jpeg','png'))]
        if not valid_attachments:
            return await analyzing_msg.edit(content="ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # Collect all images
        images = []
        for att in valid_attachments:
            image_bytes = await att.read()
            images.append(Image.open(io.BytesIO(image_bytes)).convert("RGB"))

        # Single batch inference
        results = self.classifier(images)

        # Debugging: Log the results to check the structure
        print("Debugging results:", results)

        final_label = 'SAFE'
        description_lines = []
        for idx, result in enumerate(results, start=1):
            # Ensure result is a dictionary and access keys safely
            label = 'SAFE' if result.get('label') == 'normal' else 'UNSAFE'
            if label == 'UNSAFE':
                final_label = 'UNSAFE'
            description_lines.append(
                f"ç”»åƒ {idx}:\n"
                f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å: {valid_attachments[idx-1].filename}\n"
                f"ğŸ” åˆ¤å®šãƒ©ãƒ™ãƒ«: {label} (ä¿¡é ¼åº¦: {result.get('score', 0)*100:.2f}%)\n\n"
            )

        embed = discord.Embed(
            title="NSFW ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¤å®šçµæœ",
            description=(
                "ç”»åƒã®å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n\n"
                + "\n".join(description_lines)
                + "\næœ€çµ‚åˆ¤å®šçµæœ: " + final_label
            ),
            color=discord.Color.green() if final_label == 'SAFE' else discord.Color.red()
        )
        await analyzing_msg.edit(content=None, embed=embed)

async def setup(bot):
    await bot.add_cog(NSFWDetection(bot))
